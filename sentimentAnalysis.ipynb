{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------access to tweeter api done\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from datetime import datetime, date, timedelta\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "log = pd.read_csv(\"./config/login.csv\")\n",
    "consumerKey         = log['consumerKey'][0]\n",
    "consumerSecret      = log['consumerSecret'][0]\n",
    "accessToken         = log['accessToken'][0]\n",
    "accessTokenSecret   = log['accessTokenSecret'][0]\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)\n",
    "print(\"----------------access to tweeter api done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the assigned data from tweeter\n",
      "Completed extraction for 2022-04-11 to 2022-04-12,extracted tweet count = 22.\n",
      "Completed extraction for 2022-04-12 to 2022-04-13,extracted tweet count = 48.\n",
      "Completed extraction for 2022-04-13 to 2022-04-14,extracted tweet count = 76.\n",
      "Completed extraction for 2022-04-14 to 2022-04-15,extracted tweet count = 111.\n",
      "Completed extraction for 2022-04-15 to 2022-04-16,extracted tweet count = 114.\n",
      "Completed extraction for 2022-04-16 to 2022-04-17,extracted tweet count = 124.\n",
      "Completed extraction for 2022-04-17 to 2022-04-18,extracted tweet count = 145.\n",
      "Completed extraction for 2022-04-18 to 2022-04-19,extracted tweet count = 159.\n",
      "----------------loading the data done\n"
     ]
    }
   ],
   "source": [
    "# Generate list of dates (7 days window) based on today's date\n",
    "print(\"Get the assigned data from tweeter\")\n",
    "list_of_dates = []\n",
    "today = date.today()\n",
    "for i in range(-7,1):\n",
    "    target_date = (today + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "    list_of_dates.append(target_date)\n",
    "\n",
    "list_of_dicts = []\n",
    "search_term = 'covid19 covid vaccine'\n",
    "num_tweets = 100\n",
    "\n",
    "for end_date in list_of_dates:\n",
    "    start_date = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=1)).strftime(\n",
    "        \"%Y-%m-%d\")  # Create 1-day windows for extraction\n",
    "    tweet_count = len(list_of_dicts)\n",
    "\n",
    "    for tweet in tweepy.Cursor(api.search_tweets,\n",
    "                               q=f'{search_term} since:{start_date} until:{end_date}',\n",
    "                               lang='en',\n",
    "                               count=num_tweets,\n",
    "                               tweet_mode='extended').items(num_tweets):\n",
    "        if (not tweet.retweeted) and ('RT @' not in tweet.full_text):\n",
    "            if tweet.lang == \"en\":\n",
    "                tweet_dict = {}\n",
    "                # nltk.download('vader_lexicon')\n",
    "                # sid = SentimentIntensityAnalyzer()\n",
    "                # score = sid.polarity_scores(tweet.full_text)\n",
    "                # comp = score['compound']\n",
    "                # if comp >= 0.05:\n",
    "                #     tweet_dict['tar'] = 1\n",
    "                # elif (comp >-0.05) and (comp <0.05):\n",
    "                #     tweet_dict['tar'] = 0\n",
    "                # elif comp <= -0.05:\n",
    "                #     tweet_dict['tar'] = -1\n",
    "\n",
    "                tweet_dict['username'] = tweet.user.name\n",
    "                tweet_dict['location'] = tweet.user.location\n",
    "                tweet_dict['text'] = tweet.full_text.lower()\n",
    "                # tweet_dict['fav_count'] = tweet.favorite_count\n",
    "                tweet_dict['hashtags'] = tweet.entities['hashtags']\n",
    "                tweet_dict['tweet_date'] = tweet.created_at\n",
    "                list_of_dicts.append(tweet_dict)\n",
    "                tweet_count += 1\n",
    "\n",
    "    print(f'Completed extraction for {start_date} to {end_date},extracted tweet count = {tweet_count}.')\n",
    "\n",
    "tweets_df = pd.DataFrame(list_of_dicts)\n",
    "tweets_df.sort_values(by='tweet_date').reset_index(drop=True)\n",
    "\n",
    "# Setup function to extract hashtags text from the raw hashtag dictionaries\n",
    "def extract_hashtags(hashtag_list):\n",
    "    s = \"\"  # Create empty string\n",
    "    if not hashtag_list:  # If list is empty, return empty string\n",
    "        return s\n",
    "    else:\n",
    "        for dictionary in hashtag_list:\n",
    "            s += str(dictionary['text'].lower() + ',')  # Create string (lowercase) for each hashtag text\n",
    "        s = s[:-1]  # Drop last character ','\n",
    "        return s\n",
    "\n",
    "# Extract hashtags\n",
    "tweets_df['hashtags_extracted'] = tweets_df['hashtags'].apply(lambda x: extract_hashtags(x))\n",
    "tweets_df.drop(columns='hashtags', inplace=True)\n",
    "print(\"----------------loading the data done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------cleaning the data done\n",
      "                                           username  \\\n",
      "0                                        ABNewswire   \n",
      "1                                           IARS360   \n",
      "2                                       Uncle Louie   \n",
      "3    Elisha #NoVaccineMandates #NoVaccinePassportsðŸ’‰   \n",
      "4                                    AngelsWakingUp   \n",
      "..                                              ...   \n",
      "141                                   Karl Harrison   \n",
      "142                   Maricopa County Public Health   \n",
      "143                               Chicagoland DPOCC   \n",
      "144                                  Alex Martiniuk   \n",
      "145                  KFF (Kaiser Family Foundation)   \n",
      "\n",
      "                              location  \\\n",
      "0                                        \n",
      "1                                        \n",
      "2                        New York, USA   \n",
      "3                         Carlsbad, CA   \n",
      "4                                        \n",
      "..                                 ...   \n",
      "141           British Columbia, Canada   \n",
      "142             Maricopa County, Ariz.   \n",
      "143                        Chicago, IL   \n",
      "144                                      \n",
      "145  San Francisco, CA | Washington DC   \n",
      "\n",
      "                                                  text  \\\n",
      "0    what is the risk of side effects of the covid-...   \n",
      "1    covid-19 resource newsletter - issue 127, apri...   \n",
      "2    so why are only double vaxâ€™d and double booste...   \n",
      "3    â€œâ€¦the vast majority have realized that every c...   \n",
      "4    kyle's vaccine complication https://t.co/kvysx...   \n",
      "..                                                 ...   \n",
      "141  hard to imagine any other product being allowe...   \n",
      "142  @drkeyedangel all vaccine events on our page c...   \n",
      "143  3 new #covid19 #vaccine and #reopening guides ...   \n",
      "144  effectiveness-after 4th dose #covid19 vaccine\\...   \n",
      "145  giving employees time off to get vaccinated ag...   \n",
      "\n",
      "                   tweet_date  \\\n",
      "0   2022-04-11 23:21:26+00:00   \n",
      "1   2022-04-11 23:19:14+00:00   \n",
      "2   2022-04-11 23:15:31+00:00   \n",
      "3   2022-04-11 23:07:02+00:00   \n",
      "4   2022-04-11 22:52:25+00:00   \n",
      "..                        ...   \n",
      "141 2022-04-18 21:34:33+00:00   \n",
      "142 2022-04-18 21:34:11+00:00   \n",
      "143 2022-04-18 21:33:23+00:00   \n",
      "144 2022-04-18 21:32:24+00:00   \n",
      "145 2022-04-18 21:28:00+00:00   \n",
      "\n",
      "                                    hashtags_extracted  \\\n",
      "0    familyparenting,healthmedicine,living,pharmace...   \n",
      "1                                              covid19   \n",
      "2    covid,covidisnotover,covid19,democrats,booster...   \n",
      "3    covid19,covidrestrictions,covidvaccine,covidis...   \n",
      "4    news,covid_19,omicron,covid19,athletics,athete...   \n",
      "..                                                 ...   \n",
      "141                  covid19,myocarditis,vaccineinjury   \n",
      "142                                                      \n",
      "143                 covid19,vaccine,reopening,database   \n",
      "144                                            covid19   \n",
      "145                                            covid19   \n",
      "\n",
      "                                          text_cleaned  \n",
      "0    what is the risk of side effects of the covid-...  \n",
      "1    covid-19 resource newsletter - issue  april - ...  \n",
      "2    so why are only double vaxd and double boosted...  \n",
      "3    the vast majority have realized that every cov...  \n",
      "4                       kyles vaccine complication via  \n",
      "..                                                 ...  \n",
      "141  hard to imagine any other product being allowe...  \n",
      "142  all vaccine events on our page calendar () are...  \n",
      "143                            new and guides added to  \n",
      "144  effectiveness-after th dose vaccine52 (95 ci -...  \n",
      "145  giving employees time off to get vaccinated ag...  \n",
      "\n",
      "[146 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Clean tweet text with tweet-preprocessor\n",
    "tweets_df['text_cleaned'] = tweets_df['text'].apply(lambda x: p.clean(x))\n",
    "# Remove duplicate tweets\n",
    "tweets_df.drop_duplicates(subset='text_cleaned', keep=\"first\", inplace=True)\n",
    "# Remove unnecessary characters\n",
    "# Note: Need to remove % as Stanford CoreNLP annotation encounters error if text contains some of these characters\n",
    "punct = ['%','.',',', '/', ':', '\\\\', '&amp;', '&', ';',\"\\'\"]\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "tweets_df['text_cleaned'] = tweets_df['text_cleaned'].apply(lambda x: remove_punctuations(x))\n",
    "\n",
    "# Drop tweets which have empty text field\n",
    "tweets_df['text_cleaned'].replace('', np.nan, inplace=True)\n",
    "tweets_df['text_cleaned'].replace(' ', np.nan, inplace=True)\n",
    "tweets_df.dropna(subset=['text_cleaned'], inplace=True)\n",
    "\n",
    "tweets_df = tweets_df.reset_index(drop=True)\n",
    "print(\"----------------cleaning the data done\")\n",
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writting the data into local file.....ing\n",
      "writting the data into local file.....done\n"
     ]
    }
   ],
   "source": [
    "#Write data to local file\n",
    "print(\"writting the data into local file.....ing\")\n",
    "# Create timestamp for datetime of extraction\n",
    "extract_datetime = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create csv filename\n",
    "filename = 'data/covid_vaccine_tweets_extracted_' + extract_datetime + '.csv'\n",
    "\n",
    "# Drop duplicates (if any)\n",
    "tweets_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Export dataframe as csv file with above filename\n",
    "tweets_df.to_csv(filename, index=False)\n",
    "print(\"writting the data into local file.....done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}